---
title: Transformer notes
permalink: /transformer/
---


### March 29, 2023
* Tutorials to consider going through
	* [Whisper.cpp](https://github.com/ggerganov/whisper.cpp) by George Gerganov. Speech to text on Apple Silicon.
	* Andrej Karpathy's build GPT from scratch [2-hour video](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=13s)
		* Description: 'We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.'
	* Prior videos on makemore

### March 30, 2023
* See also [page on python setup](/python)

## September 2023
### Thoughts on Transformer article
* Wikipedia article
* Original 2017 Attention is All You Need paper



### Thoughts on Llama and Llama 2 
* [Wikipedia article](https://en.wikipedia.org/wiki/LLaMA)
* [Anyscale article from August 2023](https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper)

### Overview of fine-tuning
* Single slide summarizing the entire production process. Foundation model construction, self-supervised trainingin, then the fine-tuning at the end. 
